\chapter{System Architecture}\label{sec:arch}

In this chapter, first we will introduce the job and task model in our
framework.
Then we give an overview of the proposed framework, followed by the
description and of each component.


\subsection{Job and Task Model}	%%% [DS] revised

We define our job and task models as the following.
A \emph{job} submitted by a user consists of several \emph{tasks}, which
is the minimum scheduling unit of our system.
\emph{Tasks} in the same job are independent to each other, so that they
can be processed in parallel.
A \emph{job} is finished if all the \emph{tasks} from this \emph{job} have
been processed.

Here is an example of \emph{job} and \emph{task}.
In a telecommunication company, they have to send bills to users every
month.
However, the settle days of users are not the same.
Therefore the system will batch user data with the same settle day into
a \emph{job}, and calculate the amount of money each user has to pay.
Since user data are independent to each other, they can be processed in
parallel.
We called the computation of each user data as \emph{task}.

We make some assumptions about our models.
We assume that the number of \emph{tasks} in a \emph{job} is known.
\emph{Tasks} can have different workload in a \emph{job}.
The remaining workload of a \emph{job} can be roughly estimated by
counting the number of un-processed \emph{tasks} or summing the workloads
of these \emph{tasks}.
A \emph{job} may have ``deadline'', which means all the \emph{tasks} of
this \emph{job} must be finished before this time constraint.

%%% \subsection{Job and Tasks}

%%% Before discussing our system architecture, we have to introduce the
%%% representation of workload in our system.
%%% Users will submit \emph{jobs}, each of which consists of several
%%% \emph{tasks}, the minimum unit of scheduling, to the system.
%%% As a cluster management framework, submitted jobs are meant to be
%%% expected to be processed in parallel.
%%% It is end user's responsibility to split their jobs into balanced tasks.
%%% Just like all of other parallel computing systems, the more balanced the
%%% tasks, the better the system schedules.


\subsection{System Overview}	%%% [DS] revised

\begin{figure}[htbp]
\centering
\input{figures/manage-standalone.tikz}
\caption{System Architecture Overview}
\label{fig:archi-overview}
\end{figure}

Our cloud management framework consists of three parts, the \emph{
management system}, \emph{clients}, and \emph{workers}.
The \emph{management system} is the major part of our framework.
Users submit \emph{jobs} from \emph{clients}.
\emph{Workers} are the computation nodes that process \emph{tasks} in \emph{
jobs}.
Figure~\ref{fig:archi-overview} gives the idea of the proposed
framework.
The proposed framework can work as an individual cloud computing system,
or as extension components of an existent cloud system.

\section{Client}	%%% [DS] change nothing

% Client is the interface of submission
The \emph{client} is the programming interface for users to submit jobs
to our system.
In our framework, a user will submit \emph{jobs} to the system via a
client instance created using the provided library.

% attributes
Users specify several attributes to jobs before submission, such as
deadline, priority and execution profile from previous experience.
These attributes are later passed to decision maker for reference so
that it can schedule the jobs.

% Library features: batch submission and background execution
The client library also provides several features to end users to
satisfy their varying programming needs.
First, we support background task execution, which means users can
specify the synchronization point that waits all the submitted task to
be done as they like, without blocking the code before it.
Besides, user can submit multiple jobs at a time.
Combining these two features, user can easily program the
synchronization model they want, just like those traditional work flow
description languages can do~\cite{cite:workflow-management}.
Here's an example on how to program with our client library.

% Programming example
\newfloat{Example Code}{H}{myc}
\lstloadlanguages{Ruby}
\begin{Example Code}
  \input{snippets/client-sample-code}
  \caption{Sample code of client usage}
\end{Example Code}

Assume that the user has 3 jobs --- $j_1$, $j_2$ and $j_3$ --- to be
done separately and their result to be aggregated:  $j_1$ and $j_2$
takes long time to run on the cluster can be submitted directly; $j_3$
takes shorter time on remote clusters if separated into parallel tasks
but requires time consuming local pre-processing and post-processing.
Since $j_1$, $j_2$ and pre-processing of $j_3$ is costive, it's better
to do them in parallel.
As a result, we submit $j_1$ and $j_2$ at first and wait for their
results in the background then start pre-processing $j_3$.
Not needing post-processing, results of $j_1$ and $j_2$ are synchronized
after $j_3$.
Finally, as all of the results come back, we can do the final
aggregation.

\section{Worker}	%%% [DS] revised

\emph{Workers} are the computation nodes that consume cloud resources to
process \emph{tasks}.
They execute \emph{tasks} assigned by the management system.
A \emph{worker} instance executes at most one \emph{task} at a time.
In other words, a \emph{worker} is the minimum scheduling slot of our
system.
However, it does not imply that a physical machine can run only one
\emph{task} at a time.
A physical machine can serve multiple \emph{worker} instances, therefore
executing more than one task simultaneously.

Deploying multiple \emph{worker} instances on a powerful machine (e.g.,
with large number of cores) can gain better performance from
multiprogramming, but in contrary, it might however cause resource
contention on low-end machines.
We leave the choice to system administrators.

If the task execution performance of a machine running multiple worker
instances is far worse than expected, it is very likely due to resource
contention.
In that case, the system administrator should consider reducing the
number of worker instances on that machine.
The pluggable implementation enables the reduction to be done \emph{
online}, i.e. without stopping any of other components.


\section{Management System}	%%% [DS] revised

The core part is the management system, which consists of three
components: \emph{status checker}, \emph{decision maker} and \emph{
dispatcher}.
\emph{Status checker} periodically collects the information about \emph{
worker}, and sends these information to \emph{Decision Maker}.
\emph{Decision maker} is in charge of adjusting resource allocation
plans.
\emph{Dispatcher} is the component that deal with the physical resource
allocation adjustment according to the allocation plan made by the \emph{
Decision Maker}.


\subsection{Status Checker}	%%% [DS] change nothing

Status checker periodically collects the information about \emph{worker}
instances and physical servers.
Exploiting the information collected by status checker, the decision
maker can make resource allocation plans according to a policy specified
by the system administration.

The most essential information for the system is probably the status of
workers.
A worker can be in status of either \emph{available}, \emph{occupied},
\emph{busy} or \emph{down}.
\emph{Available} means the worker is idle and ready to accept task
assignment; \emph{occupied} indicates that the worker is assigned to a
job but executing any task (e.g., waiting for necessary data); \emph{
busy} is the state that a worker is really executing a task; \emph{down}
shows that the worker instance is currently nonfunctional. 

Aside from essential worker status, system administrators can also
specify other kinds information to collect if it's essential to  their
customized policy.
For example, if someone deployed the management system on a cluster
which focuses on utilizing its I/O bandwidth, they would probably
specify a schedule policy in regard to I/O utilization of the physical
server.
In this situation, status checker must collect the information about not
only the worker instances but the physical server as well.

Necessary information will be written to a database for persistent
storage.
This allows status checker to recover its state and work normally after
being plugged off.
Besides, writing to a external database makes integration with other
framework easier since it's more general to access a database.

\subsection{Decision Maker}	%%% [DS] change nothing

Decision maker is in charge of adjusting resource allocation plans.
It makes allocation plans according to a specified \emph{policy} that
takes different parameters, for example, job deadline and priority, into
consideration. 

Instead of letting decision maker schedule resources actively, we
decided to have it work in \emph{passive mode}, which means it is
invoked only under certain circumstances.
Normally, it is only invoked by dispatcher whenever a job is submitted
or finished.
By this design, decision maker is only responsible for making allocation
plans.
In other words, the only job this component do is to perform the
schedule algorithm and give results.

Doing seemingly little work only, decision maker is however the most
critical part of the management system.
This is not only because the whole system acts according to the plan the
decision maker makes, but also because it provides the flexibility that
other frameworks can easily obtain the result of the scheduling
algorithm by invoking decision maker.

\subsection{Dispatcher}	%%% [DS] revised

\emph{Dispatcher} is the component that deal with the physical resource
allocation adjustment according to the allocation plan made by
\emph{Decision Maker}.
It receives job requests from clients, sends the job information to
\emph{Decision Maker}, and coordinates the workers and clients to run
the jobs according to the policy made by \emph{Decision Maker}.

Figure~\ref{fig:allocation-adjustment} is a nice example of adjusting
resource allocation: There were 10 workers ($w_{1\sim10}$) managed by
the system.
In the beginning, two jobs $J_1$ and $J_2$ had been allocated with 5
workers respectively ($w_{0\sim4}$, $w_{5\sim9}$), as shown in the upper
half of figure~\ref{fig:allocation-adjustment}.
After $J_3$, with which higher priority had been specified than $J_1$
and $J_2$, had been submitted, decision maker computed a new allocation
plan according to a specified policy.
In the example, the new allocation plan was 3 workers for $J_1$ and
$J_2$ each ($w_{0\sim2}$, $w_{5\sim7}$) and 4 workers ($w_{3,4,8,9}$)
for $J_3$.
Therefore, after $w_3$, $w_4$, $w_8$ and $ w_9$ had finished there task
on hand, dispatcher would told them to run tasks of $J_3$ and the
allocation plan became the lower half of
figure~\ref{fig:allocation-adjustment}.

\begin{figure}
  \centering
  \input{figures/job-reallocation.tikz}
  \caption{Job allocation adjustment}
  \label{fig:allocation-adjustment}
\end{figure}
