\chapter{System Architecture}


\begin{figure}
  \centering
  \input{figures/manage-worker.tikz}
  \caption{System Architecture Overview}
  \label{fig:archi-overview}
\end{figure}

Figure~\ref{fig:archi-overview} shows the architecture of this cloud
management system, which consists of three components: \emph{status
checker}, \emph{decision maker} and \emph{dispatcher}.  The management
system can form a complete cloud computing system by connecting with a
number of \emph{workers} or work as a part of existent cloud systems.

We implemented the management components and the worker as separate RPC
(remote procedural call) servers.  Separate RPC server implementation
makes each component \emph{pluggable}.  The pluggability gives the
system administrator flexibility to choose the most suitable component
implementation to fit different needs.  Moreover, without shutting the
whole system down, we can change system configurations, or even upgrade
the system,  by substituting target components with feasible ones.
Besides, this design allows us to easily integrate the management system
with other cluster management frameworks like JPPF~\cite{cite:JPPF} or
cloud operating system like Roystonea~\cite{cite:roystonea}.
Roystonea~\cite{cite:roystonea} also benefits from the RPC server
implementation.



\section{Worker}

Workers are the ones using resource of the cloud directly.  They execute
tasks assigned by the management system.  A worker instance executes at
most one task at a time, but it doesn't mean that a physical machine can
run one task only at a time.  One can run multiple worker instances on a
physical machine so that a physical machine could run more than one task
simultaneously.	 

Deploying multiple worker instances on a powerful machine (e.g., with
large number of cores) can gain better performance from
multiprogramming, but in contrary, it might however cause resource
contention on low-end machines.  We leave the choice to system
administrators.  

If the task execution performance of a machine running multiple worker
instances is far worse than expected, it is very likely due to resource
contention.  In that case, the system administrator should consider
reducing the number of worker instances on that machine.  Thanks to the
pluggable implementation, this can be done \emph{online}, without
stopping any of other components, even running workers we'd like to keep
on that machine, by simply stop several worker instances.

% TODO  Should I move this to another section???
In fact, our management system can function without workers.  In the
case that it is working as a part of other cluster management
frameworks, the framework in charge should be responsible for
manipulating physical resources instead.

\section{Status Checker}

The status checker periodically collects the information about worker
instances and physical servers.  Leveraging the information collected
by the status checker, the decision maker can make resource allocation
plans according to a policy specified by the user.

The most essential information for the system is probably the status of
workers.  A worker can be in status of either \emph{available},
\emph{occupied}, \emph{busy} or \emph{down}.  Available means the worker
is idle and ready to accept task assignment; occupied indicates that the
worker is assigned to a job but executing any task (e.g., waiting for
necessary data); busy is the state that a worker is really executing a
task; down shows that the worker instance is currently nonfunctional. 

Aside from essential worker status, user can also specify other kinds
information to collect if it's essential to  their customized policy.
For example, if someone deployed the management system on a cluster
which focuses on utilizing its I/O bandwidth, they would probably
specify a schedule policy in regard to I/O utilization of the
physical server.  In this situation, the status checker must collect the
information about not only the worker instances but the physical server
as well.

Necessary information will be written to a database for persistent
storage.  This allows the status checker to recover its state and work
normally after being plugged off.  Besides, writing to a external
database makes integration with other framework easier since it's more
general to access a database.

\section{Decision Maker}

Decision maker is in charge of adjusting resource allocation plans.  It
makes allocation plans according to a specified \emph{policy} that takes
different parameters, for example, job deadline and priority, into
consideration. 

Instead of letting the decision maker schedule resources actively, we
decided to have it work in \emph{passive mode}, which means it is
invoked only under certain circumstances.  Normally, it is only invoked
by the dispatcher whenever a job is submitted or finished.  By this
design, the decision maker is only responsible for making allocation
plans.  In other words, the only job this component do is to perform the
schedule algorithm and give results.  

Doing seemingly little work only, the decision maker is however the most
critical part of the system. This is not only because the system acts
according to the plan the decision maker makes, but also because it
provides the flexibility that other frameworks can easily obtain the
result of the scheduling algorithm by invoking the decision maker.

\section{Dispatcher}

Dispatcher is the component that deal with the physical resource
allocation adjustment according to the allocation plan made by the
decision maker.

\section{Client}
